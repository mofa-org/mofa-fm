# Conference Study Session with Human Participant (Mac AEC + ASR)
#
# Extends dataflow-study-audio-multi.yml with human speaker capability
# Human speaker: mac-aec → ASR → controller + bridges + audio-player
#
# Key features:
# - Human has highest priority (can interrupt)
# - Smart reset with question_id filtering
# - When human speaks: cancel LLMs, reset bridges, reset audio pipeline
# - When human finishes: restart from initial state (tutor speaks first)
# - Complete question_id propagation for data discard
# - Audio-player-based flow control: audio_complete signals replace TTS segment_complete

nodes:
  # ============ Study Participants (MaaS) ============

  - id: student1
    path: ../../target/release/dora-maas-client
    inputs:
      text: bridge-to-student1/text
      control: conference-controller/llm_control
    outputs:
      - text
      - status
      - log
    env:
      MAAS_CONFIG_PATH: study_config_maas_student1.toml
      ALIBABA_CLOUD_API_KEY: ${ALIBABA_CLOUD_API_KEY:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      DEEPSEEK_API_KEY: ${DEEPSEEK_API_KEY:-}
      LOG_LEVEL: INFO

  - id: student2
    path: ../../target/release/dora-maas-client
    inputs:
      text: bridge-to-student2/text
      control: conference-controller/llm_control
    outputs:
      - text
      - status
      - log
    env:
      MAAS_CONFIG_PATH: study_config_maas_student2.toml
      ALIBABA_CLOUD_API_KEY: ${ALIBABA_CLOUD_API_KEY:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      DEEPSEEK_API_KEY: ${DEEPSEEK_API_KEY:-}
      LOG_LEVEL: INFO

  - id: tutor
    path: ../../target/release/dora-maas-client
    inputs:
      text: bridge-to-tutor/text
      control: conference-controller/judge_prompt
    outputs:
      - text
      - status
      - log
    env:
      MAAS_CONFIG_PATH: study_config_maas_tutor.toml
      ALIBABA_CLOUD_API_KEY: ${ALIBABA_CLOUD_API_KEY:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      DEEPSEEK_API_KEY: ${DEEPSEEK_API_KEY:-}
      LOG_LEVEL: INFO

  # ============ Human Speaker Pipeline ============

  # Mac AEC + VAD Segmentation
  - id: mac-aec
    path: dynamic
    outputs:
      - audio_segment # Includes question_id metadata
      - speech_started
      - speech_ended
      - question_ended # Includes question_id metadata
      - status
      - log

  # ASR for Human Speech
  - id: asr
    build: pip install -e ../../node-hub/dora-asr
    path: dora-asr
    inputs:
      audio: mac-aec/audio_segment # Gets question_id from mac-aec
    outputs:
      - transcription # Passes question_id + adds session_status="ended"
      - status
      - log
    env:
      USE_GPU: "true"
      ASR_ENGINE: "funasr"
      LANGUAGE: "zh"
      LOG_LEVEL: "INFO"

  # ============ Multi-Participant Audio Pipeline ============

  # Multi-Input Text Segmenter with FIFO Session Queue + Smart Reset
  - id: multi-text-segmenter
    build: pip install -e ../../node-hub/dora-text-segmenter
    path: dora-text-segmenter
    inputs:
      # 3 LLM inputs (FIFO session queue)
      student1:
        source: student1/text
        queue_size: 1000
      student2:
        source: student2/text
        queue_size: 1000
      tutor:
        source: tutor/text
        queue_size: 1000

      # Audio complete signal from audio player (replaces TTS segment_complete)
      audio_complete:
        source: audio-player/audio_complete
        queue_size: 100

      # Audio buffer control for backpressure
      audio_buffer_control:
        source: audio-player/buffer_status
        queue_size: 10

      # Control signals (reset and cancel) with question_id
      control: conference-controller/llm_control
      reset: conference-controller/control_judge

    outputs:
      - text_segment_student1
      - text_segment_student2
      - text_segment_tutor
      - status
      - metrics
      - log

    env:
      SEGMENT_MODE: "sentence"
      MIN_SEGMENT_LENGTH: "5"
      MAX_SEGMENT_LENGTH: "15"
      PUNCTUATION_MARKS: '。！？.!?，,、；：""''（）【】《》'
      REMOVE_SPEAKER_ID: "true" # Remove [Name] for all 3 participants
      LOG_LEVEL: "DEBUG"

      # Buffer control thresholds
      AUDIO_BUFFER_LOW_WATER_MARK: "30" # Resume when buffer < 30%
      AUDIO_BUFFER_HIGH_WATER_MARK: "60" # Pause when buffer > 60%

  # PrimeSpeech TTS for Student1 (Daniu - Male, Rational Voice)
  - id: primespeech-student1
    build: pip install -e ../../node-hub/dora-primespeech
    path: dora-primespeech
    inputs:
      text: multi-text-segmenter/text_segment_student1
    outputs:
      - audio
      - status
      - segment_complete
      - log
    env:
      # Allow transformers to load models
      TRANSFORMERS_OFFLINE: "1"
      HF_HUB_OFFLINE: "1"

      # Voice selection - Daniu for student1
      VOICE_NAME: "Luo Xiang" # Male, rational voice
      PRIMESPEECH_MODEL_DIR: $HOME/.dora/models/primespeech

      # Language settings
      TEXT_LANG: zh
      PROMPT_LANG: zh

      # Inference parameters
      TOP_K: 5
      TOP_P: 1.0
      TEMPERATURE: 1.0
      SPEED_FACTOR: 1.0

      # Performance
      USE_GPU: false
      NUM_THREADS: 4

      RETURN_FRAGMENT: "false"
      LOG_LEVEL: DEBUG

      # Internal text segmentation
      ENABLE_INTERNAL_SEGMENTATION: "true"
      TTS_MAX_SEGMENT_LENGTH: "100"
      TTS_MIN_SEGMENT_LENGTH: "20"

  # PrimeSpeech TTS for Student2 (Doubao - Female, Emotional Voice)
  - id: primespeech-student2
    build: pip install -e ../../node-hub/dora-primespeech
    path: dora-primespeech
    inputs:
      text: multi-text-segmenter/text_segment_student2
    outputs:
      - audio
      - status
      - segment_complete
      - log
    env:
      # Allow transformers to load models
      TRANSFORMERS_OFFLINE: "1"
      HF_HUB_OFFLINE: "1"

      # Voice selection - Doubao for student2
      VOICE_NAME: "Doubao" # Female, emotional voice
      PRIMESPEECH_MODEL_DIR: $HOME/.dora/models/primespeech

      # Language settings
      TEXT_LANG: zh
      PROMPT_LANG: zh

      # Inference parameters
      TOP_K: 5
      TOP_P: 1.0
      TEMPERATURE: 1.0
      SPEED_FACTOR: 1.0

      # Performance
      USE_GPU: false
      NUM_THREADS: 4

      RETURN_FRAGMENT: "false"
      LOG_LEVEL: DEBUG

      # Internal text segmentation
      ENABLE_INTERNAL_SEGMENTATION: "true"
      TTS_MAX_SEGMENT_LENGTH: "100"
      TTS_MIN_SEGMENT_LENGTH: "20"

  # PrimeSpeech TTS for Tutor (Luo Xiang - Male, Authoritative Voice)
  - id: primespeech-tutor
    build: pip install -e ../../node-hub/dora-primespeech
    path: dora-primespeech
    inputs:
      text: multi-text-segmenter/text_segment_tutor
    outputs:
      - audio
      - status
      - segment_complete
      - log
    env:
      # Allow transformers to load models
      TRANSFORMERS_OFFLINE: "1"
      HF_HUB_OFFLINE: "1"

      # Voice selection - Luo Xiang for tutor
      VOICE_NAME: "Zhao Daniu" # Male, authoritative voice
      PRIMESPEECH_MODEL_DIR: $HOME/.dora/models/primespeech

      # Language settings
      TEXT_LANG: zh
      PROMPT_LANG: zh

      # Inference parameters
      TOP_K: 5
      TOP_P: 1.0
      TEMPERATURE: 1.0
      SPEED_FACTOR: 1.0

      # Performance
      USE_GPU: false
      NUM_THREADS: 4

      RETURN_FRAGMENT: "false"
      LOG_LEVEL: DEBUG

      # Internal text segmentation
      ENABLE_INTERNAL_SEGMENTATION: "true"
      TTS_MAX_SEGMENT_LENGTH: "100"
      TTS_MIN_SEGMENT_LENGTH: "20"

  # Multi-Input Audio Player with question_id Smart Reset
  - id: audio-player
    path: dynamic
    inputs:
      # 3 audio inputs from 3 TTS nodes
      audio_student1:
        source: primespeech-student1/audio
        queue_size: 1000
      audio_student2:
        source: primespeech-student2/audio
        queue_size: 1000
      audio_tutor:
        source: primespeech-tutor/audio
        queue_size: 1000

      # Reset signal with question_id for smart discard
      reset: conference-controller/llm_control
    outputs:
      - buffer_status # Backpressure signal to controller
      - status
      - session_start # Session start signals when first audio chunk received
      - audio_complete # Audio received signals (replaces TTS segment_complete)
      - log

  # ============ 3 Conference Bridges (Switch) ============

  # Bridge 1: Student2 + Tutor + HUMAN → Student1
  - id: bridge-to-student1
    path: ../../target/release/dora-conference-bridge
    env:
      LOG_LEVEL: INFO
      STREAMING_PORTS: student1,tutor,student2,human # ADD human
      ERROR_MESSAGE_TEMPLATE: "[{participant} is experiencing technical difficulties. We will proceed without their response.]"
      DORA_STUDY_MODE: "true"
    inputs:
      student2:
        source: student2/text
        queue_size: 1000
      tutor:
        source: tutor/text
        queue_size: 1000
      human: # NEW INPUT
        source: asr/transcription
        queue_size: 1000
      control:
        source: conference-controller/control_llm1
        queue_size: 10
    outputs:
      - text
      - status
      - log

  # Bridge 2: Student1 + Tutor + HUMAN → Student2
  - id: bridge-to-student2
    path: ../../target/release/dora-conference-bridge
    env:
      LOG_LEVEL: INFO
      STREAMING_PORTS: student1,tutor,student2,human # ADD human
      ERROR_MESSAGE_TEMPLATE: "[{participant} is experiencing technical difficulties. We will proceed without their response.]"
      DORA_STUDY_MODE: "true"
    inputs:
      student1:
        source: student1/text
        queue_size: 1000
      tutor:
        source: tutor/text
        queue_size: 1000
      human: # NEW INPUT
        source: asr/transcription
        queue_size: 1000
      control:
        source: conference-controller/control_llm2
        queue_size: 10
    outputs:
      - text
      - status
      - log

  # Bridge 3: Student1 + Student2 + HUMAN → Tutor
  - id: bridge-to-tutor
    path: ../../target/release/dora-conference-bridge
    env:
      LOG_LEVEL: INFO
      STREAMING_PORTS: student1,tutor,student2,human # ADD human
      ERROR_MESSAGE_TEMPLATE: "[{participant} is experiencing technical difficulties. We will proceed without their response.]"
      DORA_STUDY_MODE: "true"
    inputs:
      student1:
        source: student1/text
        queue_size: 1000
      student2:
        source: student2/text
        queue_size: 1000
      human: # NEW INPUT
        source: asr/transcription
        queue_size: 1000
      control:
        source: conference-controller/control_judge
        queue_size: 10
    outputs:
      - text
      - status
      - log

  # ============ Conference Controller (Brain) with Human Input ============

  - id: conference-controller
    path: ../../target/release/dora-conference-controller
    env:
      # Policy: human=priority (handled specially by controller), tutor=*, student2=1, student1=2
      # Human input triggers special reset logic in handle_human_input()
      # Use very small ratio for human since actual scheduling bypassed by controller
      DORA_POLICY_PATTERN: "[(human, 0.001), (tutor, *), (student1, 1), (student2, 1)]"
      INITIAL_QUESTION_ID: 1

      # Audio buffer backpressure control
      AUDIO_BUFFER_THRESHOLD: 30 # Pause when buffer > 30%
      AUDIO_BUFFER_RESUME_THRESHOLD: 10 # Resume when buffer < 10%
    inputs:
      # All 3 AI participants
      student1:
        source: student1/text
        queue_size: 1000
      student2:
        source: student2/text
        queue_size: 1000
      tutor:
        source: tutor/text
        queue_size: 1000

      # NEW: Human speaker (via ASR)
      human:
        source: asr/transcription
        queue_size: 1000

      # Control from study-monitor UI
      control: debate-monitor/control

      # Session start signal from audio player for round completion
      session_start: audio-player/session_start

      # Audio buffer status for backpressure
      buffer_status: audio-player/buffer_status
    outputs:
      - control_judge # Resume to bridge-to-tutor
      - control_llm2 # Resume to bridge-to-student2
      - control_llm1 # Resume to bridge-to-student1
      - llm_control # Reset/cancel to Student1 and Student2, reset to text-segmenter/audio-player
      - judge_prompt # User prompts and reset/cancel to tutor
      - status
      - log

  # ============ Study Monitor (TUI) ============

  - id: debate-monitor
    path: dynamic
    env:
      DORA_STUDY_MODE: "true"
      LOG_LEVEL: "DEBUG"
    inputs:
      # Student1 panel
      llm1_text:
        source: student1/text
        queue_size: 1000
      llm1_status: student1/status
      llm1_prompt:
        source: bridge-to-student1/text
        queue_size: 1000

      # Student2 panel
      llm2_text:
        source: student2/text
        queue_size: 1000
      llm2_status: student2/status
      llm2_prompt:
        source: bridge-to-student2/text
        queue_size: 1000

      # Tutor panel
      judge_text:
        source: tutor/text
        queue_size: 1000
      judge_status: tutor/status
      judge_prompt:
        source: bridge-to-tutor/text
        queue_size: 1000

      # NEW: Human panel
      human_text:
        source: asr/transcription
        queue_size: 1000
      human_status: asr/status

      # Audio status
      audio_status: audio-player/status
      buffer_status: audio-player/buffer_status
    outputs:
      - control

  # ============ Viewer (Logging) ============

  - id: viewer
    path: dynamic
    inputs:
      # Student1 logs
      llm1_log:
        source: student1/log
        queue_size: 1000
      llm1_status: student1/status
      llm1_text:
        source: student1/text
        queue_size: 1000

      # Student2 logs
      llm2_log:
        source: student2/log
        queue_size: 1000
      llm2_status: student2/status
      llm2_text:
        source: student2/text
        queue_size: 1000

      # Tutor logs
      judge_log:
        source: tutor/log
        queue_size: 1000
      judge_status: tutor/status
      judge_text:
        source: tutor/text
        queue_size: 1000

      # Human logs (ASR + mac-aec)
      human_text:
        source: asr/transcription
        queue_size: 1000
      asr_log:
        source: asr/log
        queue_size: 1000
      asr_status: asr/status
      mac_aec_log:
        source: mac-aec/log
        queue_size: 1000
      mac_aec_status: mac-aec/status

      # Bridge logs
      bridge1_log:
        source: bridge-to-student1/log
        queue_size: 1000
      bridge1_status: bridge-to-student1/status
      bridge1_text:
        source: bridge-to-student1/text
        queue_size: 1000
      bridge2_log:
        source: bridge-to-student2/log
        queue_size: 1000
      bridge2_status: bridge-to-student2/status
      bridge2_text:
        source: bridge-to-student2/text
        queue_size: 1000
      bridge3_log:
        source: bridge-to-tutor/log
        queue_size: 1000
      bridge3_status: bridge-to-tutor/status
      bridge3_text:
        source: bridge-to-tutor/text
        queue_size: 1000

      # Controller logs
      controller_status: conference-controller/status
      controller_log:
        source: conference-controller/log
        queue_size: 1000
      control_judge: conference-controller/control_judge
      control_llm2: conference-controller/control_llm2
      control_llm1: conference-controller/control_llm1

      # Audio pipeline logs
      segmenter_log:
        source: multi-text-segmenter/log
        queue_size: 1000
      segmenter_status: multi-text-segmenter/status
      tts1_log:
        source: primespeech-student1/log
        queue_size: 1000
      tts1_status: primespeech-student1/status
      tts2_log:
        source: primespeech-student2/log
        queue_size: 1000
      tts2_status: primespeech-student2/status
      tts3_log:
        source: primespeech-tutor/log
        queue_size: 1000
      tts3_status: primespeech-tutor/status
      audio_status: audio-player/status
      audio_player_log:
        source: audio-player/log
        queue_size: 1000
